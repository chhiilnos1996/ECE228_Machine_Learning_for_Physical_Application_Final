{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whale Detection Challenge : NN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method overview : FFT to convert the sound tracks into spectrograms, and apply distinct preprocessing methods such as clipping, noise removal, PCEN and filters. After preprocessing we feed the spectrograms into state of the art light CNN models such as Resnet 18, VGG 16 or GoogleNet to identify right whale call patterns and perform classification. We may also try the removal of pooling layers in the networks and see if it causes better outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "feature_extract = True\n",
    "groups = 3\n",
    "vis_batch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x+\"_prep_10/\"), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=8) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The small CNN model\n",
    "class small_model(nn.Module):\n",
    "    # input size 40*500\n",
    "    # image = cv2.resize(image, (40, 500), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(small_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=7, stride=1, padding=(3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=7, stride=1, padding=(3, 3), bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=(2, 2), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=(2, 2), bias=False)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=(1, 1), bias=False)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.bn4 = nn.BatchNorm1d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.bn6 = nn.BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.fc4 = nn.Linear(1024, 2)\n",
    "        self.bn7 = nn.BatchNorm1d(2, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn1(self.conv2(x)))\n",
    "        x = F.relu(self.bn1(self.conv2(x)))\n",
    "        x = self.dropout(self.pool(x))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "        x = F.relu(self.bn2(self.conv4(x)))\n",
    "        x = F.relu(self.bn2(self.conv4(x)))\n",
    "        x = self.dropout(self.pool(x))\n",
    "    \n",
    "        x = F.relu(self.bn3(self.conv5(x)))\n",
    "        x = F.relu(self.bn3(self.conv6(x)))\n",
    "        x = F.relu(self.bn3(self.conv6(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "               \n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = F.relu(self.bn6(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_model(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  (dropout): Dropout2d(p=0.3, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (conv5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (flat): Flatten()\n",
      "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (bn4): BatchNorm1d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (bn5): BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (bn6): BatchNorm1d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (bn7): BatchNorm1d(2, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = small_model()\n",
    "#     num_ftrs = model_ft.fc3.out_features\n",
    "#     model_ft.fc4 = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = (64, 64)  \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    history = {'train_loss':[],'train_acc':[],'val_loss':[],'val_acc':[]}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc)\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t conv2.weight\n",
      "\t conv3.weight\n",
      "\t bn2.weight\n",
      "\t bn2.bias\n",
      "\t conv4.weight\n",
      "\t conv5.weight\n",
      "\t bn3.weight\n",
      "\t bn3.bias\n",
      "\t conv6.weight\n",
      "\t fc1.weight\n",
      "\t fc1.bias\n",
      "\t bn4.weight\n",
      "\t bn4.bias\n",
      "\t fc2.weight\n",
      "\t fc2.bias\n",
      "\t bn5.weight\n",
      "\t bn5.bias\n",
      "\t fc3.weight\n",
      "\t fc3.bias\n",
      "\t bn6.weight\n",
      "\t bn6.bias\n",
      "\t fc4.weight\n",
      "\t fc4.bias\n",
      "\t bn7.weight\n",
      "\t bn7.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.4880 Acc: 0.7746\n",
      "val Loss: 0.5676 Acc: 0.7660\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4040 Acc: 0.8757\n",
      "val Loss: 0.5109 Acc: 0.7660\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.3218 Acc: 0.8873\n",
      "val Loss: 0.6354 Acc: 0.7660\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.2528 Acc: 0.8988\n",
      "val Loss: 0.6777 Acc: 0.7660\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9191\n",
      "val Loss: 0.6186 Acc: 0.7660\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.1931 Acc: 0.9335\n",
      "val Loss: 0.6083 Acc: 0.7663\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.1396 Acc: 0.9422\n",
      "val Loss: 0.5641 Acc: 0.7663\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9509\n",
      "val Loss: 0.5960 Acc: 0.7660\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.1185 Acc: 0.9451\n",
      "val Loss: 0.5079 Acc: 0.7730\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9509\n",
      "val Loss: 0.5406 Acc: 0.7677\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.1026 Acc: 0.9480\n",
      "val Loss: 0.4695 Acc: 0.7690\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9566\n",
      "val Loss: 0.5556 Acc: 0.7667\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9711\n",
      "val Loss: 0.6312 Acc: 0.7667\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.9653\n",
      "val Loss: 0.6253 Acc: 0.7677\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9682\n",
      "val Loss: 0.4434 Acc: 0.8000\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.1128 Acc: 0.9624\n",
      "val Loss: 0.4900 Acc: 0.8050\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.0890 Acc: 0.9653\n",
      "val Loss: 0.5443 Acc: 0.7833\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.0832 Acc: 0.9682\n",
      "val Loss: 0.4558 Acc: 0.7970\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9682\n",
      "val Loss: 0.4874 Acc: 0.8003\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.0730 Acc: 0.9740\n",
      "val Loss: 0.9435 Acc: 0.7667\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.0534 Acc: 0.9798\n",
      "val Loss: 0.7215 Acc: 0.7740\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.0632 Acc: 0.9740\n",
      "val Loss: 0.7377 Acc: 0.7697\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.0559 Acc: 0.9798\n",
      "val Loss: 0.5524 Acc: 0.7890\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.0653 Acc: 0.9798\n",
      "val Loss: 0.3548 Acc: 0.8540\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.0482 Acc: 0.9855\n",
      "val Loss: 0.4801 Acc: 0.7890\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9595\n",
      "val Loss: 0.3515 Acc: 0.8497\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 0.9769\n",
      "val Loss: 0.5460 Acc: 0.7763\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 0.9913\n",
      "val Loss: 0.3554 Acc: 0.8520\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.0279 Acc: 0.9884\n",
      "val Loss: 0.3555 Acc: 0.8533\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.0352 Acc: 0.9855\n",
      "val Loss: 0.4216 Acc: 0.8267\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.0389 Acc: 0.9855\n",
      "val Loss: 0.3563 Acc: 0.8553\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.0281 Acc: 0.9913\n",
      "val Loss: 0.3498 Acc: 0.8663\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.0263 Acc: 0.9884\n",
      "val Loss: 0.3932 Acc: 0.8503\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.0417 Acc: 0.9855\n",
      "val Loss: 0.3632 Acc: 0.8603\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.0459 Acc: 0.9827\n",
      "val Loss: 0.3782 Acc: 0.8587\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.0319 Acc: 0.9913\n",
      "val Loss: 0.3612 Acc: 0.8703\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.0249 Acc: 0.9942\n",
      "val Loss: 0.3753 Acc: 0.8590\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.0213 Acc: 0.9971\n",
      "val Loss: 0.3525 Acc: 0.8623\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.0642 Acc: 0.9827\n",
      "val Loss: 0.3656 Acc: 0.8640\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9855\n",
      "val Loss: 0.3878 Acc: 0.8757\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 1.0000\n",
      "val Loss: 0.4187 Acc: 0.8393\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.0531 Acc: 0.9769\n",
      "val Loss: 0.5335 Acc: 0.7947\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.0239 Acc: 0.9913\n",
      "val Loss: 0.5301 Acc: 0.7960\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.0175 Acc: 0.9971\n",
      "val Loss: 0.4817 Acc: 0.8170\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.0297 Acc: 0.9913\n",
      "val Loss: 0.3813 Acc: 0.8573\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.0313 Acc: 0.9913\n",
      "val Loss: 0.4795 Acc: 0.8140\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.0156 Acc: 0.9942\n",
      "val Loss: 0.4341 Acc: 0.8293\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.0208 Acc: 0.9884\n",
      "val Loss: 0.4366 Acc: 0.8340\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.0258 Acc: 0.9971\n",
      "val Loss: 0.4484 Acc: 0.8327\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9971\n",
      "val Loss: 0.4242 Acc: 0.8353\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 1.0000\n",
      "val Loss: 0.3962 Acc: 0.8470\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9855\n",
      "val Loss: 0.5543 Acc: 0.7973\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.0172 Acc: 0.9971\n",
      "val Loss: 0.3526 Acc: 0.8707\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.0175 Acc: 0.9971\n",
      "val Loss: 0.4329 Acc: 0.8713\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 1.0000\n",
      "val Loss: 0.3913 Acc: 0.8737\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.0202 Acc: 0.9942\n",
      "val Loss: 0.3840 Acc: 0.8707\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.9942\n",
      "val Loss: 0.4205 Acc: 0.8743\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9971\n",
      "val Loss: 0.4152 Acc: 0.8733\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9942\n",
      "val Loss: 0.4090 Acc: 0.8727\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.0102 Acc: 0.9971\n",
      "val Loss: 0.4026 Acc: 0.8680\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.9971\n",
      "val Loss: 0.4169 Acc: 0.8733\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 1.0000\n",
      "val Loss: 0.4393 Acc: 0.8733\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.0132 Acc: 0.9971\n",
      "val Loss: 0.4863 Acc: 0.8703\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 1.0000\n",
      "val Loss: 0.4016 Acc: 0.8737\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.0160 Acc: 0.9942\n",
      "val Loss: 0.3911 Acc: 0.8727\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 1.0000\n",
      "val Loss: 0.4007 Acc: 0.8723\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.0041 Acc: 1.0000\n",
      "val Loss: 0.3903 Acc: 0.8683\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9942\n",
      "val Loss: 0.4251 Acc: 0.8693\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.0189 Acc: 0.9913\n",
      "val Loss: 0.4254 Acc: 0.8707\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.0040 Acc: 1.0000\n",
      "val Loss: 0.3831 Acc: 0.8700\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.0171 Acc: 0.9971\n",
      "val Loss: 0.4080 Acc: 0.8773\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.0073 Acc: 0.9971\n",
      "val Loss: 0.4423 Acc: 0.8773\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.0100 Acc: 1.0000\n",
      "val Loss: 0.4304 Acc: 0.8727\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.0079 Acc: 1.0000\n",
      "val Loss: 0.4548 Acc: 0.8743\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 1.0000\n",
      "val Loss: 0.4882 Acc: 0.8733\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 1.0000\n",
      "val Loss: 0.5751 Acc: 0.8707\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 1.0000\n",
      "val Loss: 0.4780 Acc: 0.8773\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 1.0000\n",
      "val Loss: 0.4031 Acc: 0.8767\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 1.0000\n",
      "val Loss: 0.4169 Acc: 0.8757\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 1.0000\n",
      "val Loss: 0.4669 Acc: 0.8740\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9971\n",
      "val Loss: 0.4434 Acc: 0.8737\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.0030 Acc: 1.0000\n",
      "val Loss: 0.4457 Acc: 0.8707\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9971\n",
      "val Loss: 0.4225 Acc: 0.8737\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 1.0000\n",
      "val Loss: 0.4382 Acc: 0.8737\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 0.4249 Acc: 0.8723\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.0031 Acc: 1.0000\n",
      "val Loss: 0.4271 Acc: 0.8733\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 1.0000\n",
      "val Loss: 0.4225 Acc: 0.8740\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 0.4208 Acc: 0.8737\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 1.0000\n",
      "val Loss: 0.4482 Acc: 0.8750\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 0.4460 Acc: 0.8743\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 1.0000\n",
      "val Loss: 0.4715 Acc: 0.8720\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 1.0000\n",
      "val Loss: 0.4532 Acc: 0.8717\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.0113 Acc: 0.9971\n",
      "val Loss: 0.4400 Acc: 0.8670\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.0183 Acc: 0.9913\n",
      "val Loss: 0.5210 Acc: 0.8687\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.9971\n",
      "val Loss: 0.4704 Acc: 0.8743\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9971\n",
      "val Loss: 0.4484 Acc: 0.8730\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.4405 Acc: 0.8757\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 0.4746 Acc: 0.8753\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.4796 Acc: 0.8743\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.0138 Acc: 0.9971\n",
      "val Loss: 0.4720 Acc: 0.8733\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 1.0000\n",
      "val Loss: 0.4668 Acc: 0.8720\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 1.0000\n",
      "val Loss: 0.4914 Acc: 0.8717\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 1.0000\n",
      "val Loss: 0.4706 Acc: 0.8697\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.4819 Acc: 0.8693\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 1.0000\n",
      "val Loss: 0.5229 Acc: 0.8693\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.0054 Acc: 0.9971\n",
      "val Loss: 0.4276 Acc: 0.8700\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 0.3948 Acc: 0.8673\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.4092 Acc: 0.8680\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.0044 Acc: 1.0000\n",
      "val Loss: 0.4510 Acc: 0.8703\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.9971\n",
      "val Loss: 0.4513 Acc: 0.8677\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 0.4362 Acc: 0.8660\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.9971\n",
      "val Loss: 0.4318 Acc: 0.8630\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 1.0000\n",
      "val Loss: 0.4364 Acc: 0.8640\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 1.0000\n",
      "val Loss: 0.4147 Acc: 0.8640\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.0065 Acc: 1.0000\n",
      "val Loss: 0.4050 Acc: 0.8637\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.0148 Acc: 0.9971\n",
      "val Loss: 0.4202 Acc: 0.8477\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9971\n",
      "val Loss: 0.4074 Acc: 0.8577\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 1.0000\n",
      "val Loss: 0.4599 Acc: 0.8743\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.9971\n",
      "val Loss: 0.4895 Acc: 0.8743\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 1.0000\n",
      "val Loss: 0.4756 Acc: 0.8740\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 1.0000\n",
      "val Loss: 0.4406 Acc: 0.8707\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 0.4487 Acc: 0.8693\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 1.0000\n",
      "val Loss: 0.4411 Acc: 0.8670\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 0.4638 Acc: 0.8700\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.9971\n",
      "val Loss: 0.4646 Acc: 0.8703\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 1.0000\n",
      "val Loss: 0.4309 Acc: 0.8650\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 1.0000\n",
      "val Loss: 0.4754 Acc: 0.8700\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 1.0000\n",
      "val Loss: 0.5103 Acc: 0.8700\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 1.0000\n",
      "val Loss: 0.4840 Acc: 0.8733\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.4973 Acc: 0.8737\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 1.0000\n",
      "val Loss: 0.5453 Acc: 0.8730\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 0.4506 Acc: 0.8697\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.4590 Acc: 0.8723\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.9942\n",
      "val Loss: 0.4450 Acc: 0.8720\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 1.0000\n",
      "val Loss: 0.4269 Acc: 0.8630\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 1.0000\n",
      "val Loss: 0.4388 Acc: 0.8647\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 0.4336 Acc: 0.8650\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 1.0000\n",
      "val Loss: 0.4430 Acc: 0.8697\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 0.4426 Acc: 0.8653\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 1.0000\n",
      "val Loss: 0.4516 Acc: 0.8680\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 0.4509 Acc: 0.8687\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 0.4440 Acc: 0.8673\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0062 Acc: 0.9971\n",
      "val Loss: 0.4312 Acc: 0.8670\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 0.4356 Acc: 0.8630\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.0060 Acc: 1.0000\n",
      "val Loss: 0.4365 Acc: 0.8607\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 0.4441 Acc: 0.8567\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.4437 Acc: 0.8677\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 1.0000\n",
      "val Loss: 0.4233 Acc: 0.8680\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 1.0000\n",
      "val Loss: 0.4253 Acc: 0.8697\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 0.4188 Acc: 0.8700\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 0.4359 Acc: 0.8700\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 1.0000\n",
      "val Loss: 0.4430 Acc: 0.8730\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 0.4696 Acc: 0.8730\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 0.4670 Acc: 0.8740\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.4406 Acc: 0.8740\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.4838 Acc: 0.8743\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 0.4507 Acc: 0.8747\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 1.0000\n",
      "val Loss: 0.4362 Acc: 0.8740\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 0.4437 Acc: 0.8757\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9971\n",
      "val Loss: 0.4294 Acc: 0.8733\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 1.0000\n",
      "val Loss: 0.4239 Acc: 0.8720\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0038 Acc: 1.0000\n",
      "val Loss: 0.4218 Acc: 0.8690\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 0.4231 Acc: 0.8660\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 1.0000\n",
      "val Loss: 0.4373 Acc: 0.8687\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "val Loss: 0.4329 Acc: 0.8670\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 0.4320 Acc: 0.8667\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 1.0000\n",
      "val Loss: 0.4439 Acc: 0.8700\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0028 Acc: 1.0000\n",
      "val Loss: 0.4529 Acc: 0.8693\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 1.0000\n",
      "val Loss: 0.4480 Acc: 0.8683\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 1.0000\n",
      "val Loss: 0.4322 Acc: 0.8617\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "val Loss: 0.4512 Acc: 0.8643\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 1.0000\n",
      "val Loss: 0.4608 Acc: 0.8650\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 1.0000\n",
      "val Loss: 0.4738 Acc: 0.8687\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 1.0000\n",
      "val Loss: 0.4780 Acc: 0.8697\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.0034 Acc: 0.9971\n",
      "val Loss: 0.4669 Acc: 0.8667\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 0.9913\n",
      "val Loss: 0.4601 Acc: 0.8700\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.9971\n",
      "val Loss: 0.4423 Acc: 0.8590\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0083 Acc: 0.9971\n",
      "val Loss: 0.4790 Acc: 0.8697\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 1.0000\n",
      "val Loss: 0.4849 Acc: 0.8693\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 1.0000\n",
      "val Loss: 0.4882 Acc: 0.8707\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 1.0000\n",
      "val Loss: 0.5409 Acc: 0.8717\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.0027 Acc: 1.0000\n",
      "val Loss: 0.5438 Acc: 0.8707\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.0043 Acc: 1.0000\n",
      "val Loss: 0.4625 Acc: 0.8690\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 1.0000\n",
      "val Loss: 0.4395 Acc: 0.8660\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist, best_acc = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)\n",
    "torch.save(model_ft.state_dict(), 'model_weight/NN_model_'+str(num_epochs))\n",
    "print(model_ft.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.keys())\n",
    "\n",
    "#### Fill in plot #####\n",
    "#Plot accuracy vs epoch\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(hist['train_acc'])\n",
    "plt.plot(hist['val_acc'])\n",
    "plt.title('accuracy : NN_model')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot loss vs epoch\n",
    "plt.subplot(212)\n",
    "plt.plot(hist['train_loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.title('loss : NN_model')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_vis(vis_loader, groups):\n",
    "    since = time.time()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model = models.googlenet(aux_logits=False)\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    # load best model weights\n",
    "    state_dict = torch.load('model_weight/NN_model_'+str(num_epochs))\n",
    "    print(state_dict.keys())\n",
    "    model.load_state_dict(torch.load('model_weight/NN_model_'+str(num_epochs)))\n",
    "    model.to(device)\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    # Iterate over data.\n",
    "    cnt=0\n",
    "    vis_dict = {'inputs':np.empty([groups,vis_batch,3,224,224]),'outputs':np.empty([groups,vis_batch,num_classes]),'labels':np.empty([groups,vis_batch]),'preds':np.empty([groups,vis_batch])}\n",
    "    for inputs, labels in vis_loader:\n",
    "            if(cnt==groups):\n",
    "                break;\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            vis_dict['inputs'][cnt,:,:,:,:]=inputs.cpu().numpy()\n",
    "            vis_dict['labels'][cnt,:]=labels.cpu().numpy()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(False):\n",
    "            # Get model outputs and calculate loss\n",
    "            # Special case for inception because in training it has an auxiliary output. In train\n",
    "            #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "            #   but in testing we only consider the final output.\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                vis_dict['outputs'][cnt,:,:]=outputs.cpu().numpy()\n",
    "                vis_dict['preds'][cnt,:]=preds.cpu().numpy()\n",
    "            cnt+=1\n",
    "            \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print('Inference complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return vis_dict\n",
    "\n",
    "vis_loader = torch.utils.data.DataLoader(image_datasets['val'], batch_size=vis_batch, shuffle=True, num_workers=8)\n",
    "vis_dict = forward_vis(vis_loader, groups)\n",
    "print(vis_dict['inputs'].shape)\n",
    "print(vis_dict['outputs'].shape)\n",
    "print(vis_dict['labels'].shape)\n",
    "print(vis_dict['preds'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-06824ed198f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvis_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vis_dict' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(groups) :\n",
    "    for j in range(vis_batch) :\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.subplot(groups,vis_batch,4*i+j+1)\n",
    "        image = np.moveaxis(np.squeeze(vis_dict['inputs'][i,j]), 0, -1)\n",
    "        image = (image-np.amin(image))/(np.amax(image)-np.amin(image))\n",
    "        plt.imshow(image)\n",
    "        plt.tick_params(\n",
    "        axis='both',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        left=False,         # ticks along the left edge are off\n",
    "        labelbottom=False,\n",
    "        labelleft=False) # labels along the bottom edge are off\n",
    "        plt.title('preprocessed '+str(i)+' '+str(j))\n",
    "        plt.xlabel('label : '+str(vis_dict['labels'][i,j])+', pred : '+str(vis_dict['preds'][i,j])+', output : '+str(vis_dict['outputs'][i,j,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
